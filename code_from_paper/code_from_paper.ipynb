{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e10ee6b1-06f2-49a6-90c7-d4b5db327158",
   "metadata": {},
   "source": [
    "## Generate code from the methods section from ML Research Papers using generative AI and Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dd73d7-22b9-479d-bbdd-fc92e726a215",
   "metadata": {},
   "source": [
    "Data Scientists often need to confer with research papers as they may contain solutions to their problems. But sometimes, especially for students, these papers may require a lot of prior knowledge to implement the steps being highlighted. Generative AI can be utilized to get a head start on how to implement the methods being outlined in their programming language of choice. This can often be a good stepping stone for understanding the methodology being used to solve the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b4de710b-d145-4d35-b3a1-9c1a00fc16c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Lanchain modules\n",
    "from langchain.vectorstores.cassandra import Cassandra\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings \n",
    "\n",
    "# support for dataset retrieval with Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "# CassIO, engine powering Astra DB integration in LangChain\n",
    "import cassio\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "from typing_extensions import Concatenate\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "96c9a646-9517-4f29-8229-2ee91d3714b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using open AI's model\n",
    "import os\n",
    "import openai\n",
    "\n",
    "\n",
    "# create a .env file which contains personal OpenAI api key, Astra DB token, Astra DB ID (obtained from DATASTAX) \n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5540fbe2-b75c-40a5-a2a7-fdfa5a15bc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# initialize connection to db\n",
    "cassio.init(token=os.environ['ASTRA_DB_APPLICATION_TOKEN'], database_id=os.environ['ASTRA_DB_ID'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "cf77daa0-fe21-4493-af46-5deafaebc9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract raw text from each page when in the methods section - will limit the embeddings stored in vector db\n",
    "\n",
    "from typing_extensions import Concatenate\n",
    "\n",
    "def extract_method_from_research(pdf_path):\n",
    "    \"\"\"Extarcts the method section from a pdf\"\"\"\n",
    "\n",
    "    pdfreader = PdfReader(pdf_path)\n",
    "\n",
    "    raw_txt = ''    \n",
    "    \n",
    "    for i, page in enumerate(pdfreader.pages):\n",
    "        content = page.extract_text()\n",
    "        \n",
    "        # check if the page contains the start of the methods section - not fool-proof\n",
    "        if \"methods\" in content.lower():\n",
    "            raw_txt += content\n",
    "        elif \"results\" in content.lower():\n",
    "            break\n",
    "            \n",
    "    return raw_txt\n",
    "\n",
    "# not completely fool-proof method\n",
    "raw_txt = extract_method_from_research('cv_paper.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e5412ac2-1512-4e40-b009-ee1fbc22d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LangChain embedding and LLM objects\n",
    "\n",
    "llm = OpenAI()\n",
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4c485d4b-1cc8-49ca-8455-55c7aa8cca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create langchain vector store backed by Astra DB\n",
    "\n",
    "astra_vector_store = Cassandra(\n",
    "    embedding=embedding,\n",
    "    table_name=\"pdf_code\",\n",
    "    session=None,\n",
    "    keyspace=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8f8f362d-a954-44b8-908f-ca763e639005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunking data and converting chunks to vectors\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator='\\n',\n",
    "    chunk_size=800,\n",
    "    chunk_overlap = 200,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_text(raw_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "4485e48d-c806-4384-9aaf-6334b8488c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yuet al. EURASIP Journal on Image and Video Processing 2013, 2013 :52 Page 2 of 10\\nhttp://jivp.eurasipjournals.com/content/2013/1/52\\ncollection, we selected sequences and species to keep the\\ndata balanced. Then, we manually cropped animals from\\nall the frames to generate a dataset with 7, 196 images over\\n18 different vertebrate species.\\n2 Related work\\nMost related works are camera-based studies of wildlife\\nthat use image analysis to identify individual animals of\\nselect species with unique coat patterns (e.g., spots or\\nstripes). Bolger et al. [10] applied software to help identify\\nindividual animals based on coat patterns for subsequent\\nphotographic mark-recapture analysis. The data they used\\nwas image based, which is a cost-effective, non-invasive',\n",
       " 'individual animals based on coat patterns for subsequent\\nphotographic mark-recapture analysis. The data they used\\nwas image based, which is a cost-effective, non-invasive\\nway to study population. The method they used was the\\nSIFT key points extraction and matching. Thus, they\\nonly focused on individual animal identification for these\\nstrongly marked texture species.\\nIdentifying species from remote camera images remains\\na major challenge that has not been addressed. In the com-\\nmunity of computer vision, there exist a lot of methods to\\nrecognize general object. One of the most successful ones\\nis Yang’s work [6], in which ScSPM is applied. Spatial pyra-\\nmid matching (SPM) with max pooling [11] can not only\\nmodel the spatial layout of local image features, but also',\n",
       " 'is Yang’s work [6], in which ScSPM is applied. Spatial pyra-\\nmid matching (SPM) with max pooling [11] can not only\\nmodel the spatial layout of local image features, but also\\nachieve translation invariance of animal body. As being\\neasy and simple to construct, the SPM kernel turns out to\\nbe highly effective in practice [12]. Sparse coding has been\\nsuccessfully applied to model local features, and to con-\\nstruct overcomplete dictionary that can sparsely represent\\nthe local features. Sparse coding can yield better results\\nthan vector quantization and hard assignment [6].3 Materials and methods\\nOur pattern extraction and classification program is based\\non the ScSPM [6], as shown in Figure 1. The algorithm\\nfirst extracts local feature descriptor densely. We combine',\n",
       " 'Our pattern extraction and classification program is based\\non the ScSPM [6], as shown in Figure 1. The algorithm\\nfirst extracts local feature descriptor densely. We combine\\ntwo kinds of local descriptors: SIFT and cLBP . In order to\\nsparsely represent local features, the dictionary is learned\\nvia weighted sparse coding, for each kind of descriptor\\nfeature. Similar local features can generate similar codes\\nafter sparse coding on the dictionary, which is essential for\\nrecognition because it retains discriminative information\\nwhile suppressing the noise. Finally, max pooling using\\nSPM is used to construct the global image feature that\\nconverts an image or a bounding box to a single vector. We\\nthen apply linear multi-class SVMs to classify the global',\n",
       " 'SPM is used to construct the global image feature that\\nconverts an image or a bounding box to a single vector. We\\nthen apply linear multi-class SVMs to classify the global\\nfeature to one category of species, assuming SVMs are\\ntrained beforehand using training data.\\n3.1 Local feature extraction\\nThe camera-trap images contain rich noise and clut-\\nter. This requires us to develop a both discriminant and\\ninvariant local feature to describe local image patches.\\nDense SIFT feature, also known as dense histogram of ori-\\nented gradients, is successfully used in some recognition\\nwork. SIFT descriptor is invariant to moderate scaling and\\nshifting change of edges and linear illuminance variation\\nin image patch; however, it fails when nonlinear illumi-',\n",
       " 'work. SIFT descriptor is invariant to moderate scaling and\\nshifting change of edges and linear illuminance variation\\nin image patch; however, it fails when nonlinear illumi-\\nnance change occurs. cLBP , in contrast, is the perfect local\\ntexture descriptor that is invariant to moderate nonlin-\\near illuminance variation. In the area of computer vision,\\nfor human detection [13], HOG and cLBP features are\\nconcatenated to obtain the final feature. But the simple\\nImageLocal featureFeature map\\nLocal feature\\nextractionFeature coding. . .Global feature\\nFeature pooling\\nFigure 1 The architecture of ScSPM algorithm. The densely extracted local features are pooled across different spatial locations over different',\n",
       " 'Feature pooling\\nFigure 1 The architecture of ScSPM algorithm. The densely extracted local features are pooled across different spatial locations over different\\nspatial scales.Yuet al. EURASIP Journal on Image and Video Processing 2013, 2013 :52 Page 5 of 10\\nhttp://jivp.eurasipjournals.com/content/2013/1/52\\nFigure 3 Two sequences of agouti and collared peccary captured in day and night.\\nTable 2 The 18 terrestrial species, captured by camera\\ntraps in Panama and the Netherlands\\nCommon name Latin name Pictures ( n)S i t e\\nAgouti Dasyprocta punctata 518 Panama\\nPaca Cuniculus paca 285 Panama\\nCollared peccary DTayassu tajacu 263 Panama\\nRed brocket deer Mazama americana 297 Panama\\nWhite-nosed coati Nasua narica 325 Panama\\nSpiny rat Proechimys semispinosus 175 Panama',\n",
       " 'Collared peccary DTayassu tajacu 263 Panama\\nRed brocket deer Mazama americana 297 Panama\\nWhite-nosed coati Nasua narica 325 Panama\\nSpiny rat Proechimys semispinosus 175 Panama\\nOcelot Leopardus pardalis 184 Panama\\nRed-tailed squirrel Sciurus granatensis 143 Panama\\nCommon opossum Didelphis marsupialis 264 Panama\\nGreat tinamou Tinamus major 350 Panama\\nWhite-tailed deer Odocoileus virginianus 1,091 Panama\\nMouflon Apodemus sylvaticus 896 Holland\\nRed deer Cervus elaphus 802 Holland\\nRoe deer Capreolus capreolus 362 Holland\\nWild boar Sus scrofa 487 Holland\\nRed fox Vulpes vulpes 120 Holland\\nEuropean hare Lepus europaeus 176 Holland\\nWood mouse Apodemus sylvaticus 455 Holland\\nImages were used to test the recognition algorithm.where yc\\ni=1i fyi=c,o t h e r w i s e yc\\ni=−1, and l(wc;yc\\ni,zi)',\n",
       " 'European hare Lepus europaeus 176 Holland\\nWood mouse Apodemus sylvaticus 455 Holland\\nImages were used to test the recognition algorithm.where yc\\ni=1i fyi=c,o t h e r w i s e yc\\ni=−1, and l(wc;yc\\ni,zi)\\nis the hinge loss function. The standard hinge loss func-\\ntion is not differentiable everywhere, but here we can\\nuse quadratic hinge loss as below instead to make use of\\ngradient-based optimization methods, e.g., LBFGS [6].\\nl(wc;yc\\ni,zi)=[m a x(0, 1−wT\\ncz·yc\\ni)]2\\n4 Experimental results\\n4.1 Data set\\nWe used images of wildlife captured with motion-\\nsensitive camera traps (Reconyx RC55, PC800 and HC500,\\nHolmen, WI, USA), which generate sequences of 3.1\\nMegapixel JPEG images at about 1 frame/s upon trig-\\ngering by an infrared motion sensor. Color images are',\n",
       " 'Holmen, WI, USA), which generate sequences of 3.1\\nMegapixel JPEG images at about 1 frame/s upon trig-\\ngering by an infrared motion sensor. Color images are\\ncaptured during the day and gray-scale images are cap-\\ntured at night using and an infrared flash, which is invis-\\nible to most animals. We used images from tropical rain\\nforest (Barro Colorado Island, Panama) and temperate\\nforest and heathland (Hoge Veluwe National Park, the\\nNetherlands). Expert zoologists identified the animals in\\nthe images. We did not edit the data set for ease of identi-\\nfication, so it includes many of the typical challenges faced\\nby camera trapping data, including cases where the animal\\nis too small or is occluded by vegetation.']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a13b0d98-fa07-4f06-8306-31d6617a62ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 10 headlines.\n"
     ]
    }
   ],
   "source": [
    "# load top 50 text into database for vector store  - it converts to embeddings\n",
    "\n",
    "astra_vector_store.add_texts(texts)\n",
    "\n",
    "print(f\"Inserted {len(texts)} headlines.\" )\n",
    "\n",
    "astra_vector_index = VectorStoreIndexWrapper(vectorstore=astra_vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5150a67-aebe-4549-bf08-15dcc764e957",
   "metadata": {},
   "source": [
    "### Run Query to obtain detail summaryof methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "fd971ae5-8e10-4923-a428-632061609854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: 1. The researchers are using a cost-effective and non-invasive method to study population, which involves identifying individual animals based on their coat patterns for subsequent photographic mark-recapture analysis.\n",
      "2. The data used for the study is image-based.\n",
      "3. The method used for identifying individual animals is the SIFT key points extraction and matching.\n",
      "4. This method is specifically focused on identifying strongly marked texture species.\n",
      "5. The researchers acknowledge that identifying species from remote camera images is a major challenge that has not been addressed.\n",
      "6. In the community of computer vision, there are various methods for recognizing general objects.\n",
      "7. One of the most successful methods is Yang's work, which uses ScSPM (Spatial pyramid matching).\n",
      "8. The ScSPM is applied in the researchers' pattern extraction and classification program.\n",
      "9. The algorithm first extracts local feature descriptors densely.\n",
      "10. Two kinds of local descriptors, SIFT and cLBP, are combined for sparsely representing the local features.\n",
      "11. The dictionary is learned through weighted sparse coding for each kind of descriptor feature.\n",
      "12. Similar local features can generate similar codes after sparse coding, which is important for recognition as it retains discriminative information while suppressing noise.\n",
      "13. Max pooling using SPM is then used to construct a global image feature, which converts an image or a bounding box to a single vector.\n",
      "14. Linear multi-class SVMs are applied to classify the global image feature.\n",
      "15. This summary outlines the methods used in a chronological manner and provides enough detail for reproducibility.\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0.9, max_tokens=-1) # initialize open AI chat with 0.9 temp to give LLM freedom for creativity, \n",
    "                                            # Setting max_tokens to -1 to prevent cut responses\n",
    "                                            # Please remove the max_tokens to not incur heavy costs from your API\n",
    "\n",
    "query_text = \"\"\"This text corresponds to the methods section of a research paper. Please provide a detailed summary of all the methods being utilized \\\n",
    "chronologically without missing any steps. It is important that the summary is detailed and summarizes each of the methods \\\n",
    "being outlined independently. It is also important the the summary outline methods in such a way that they are reproducible.\"\"\"\n",
    "\n",
    "summary = astra_vector_index.query(query_text, llm=llm).strip()\n",
    "print(f\"Summary: {summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "4ecc14ab-96e1-41f7-88de-2e2407a4ec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate # the prompt template\n",
    "from langchain.chains import LLMChain # LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "e1d7f69e-7601-4184-85ad-319dccd17a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_string = \"\"\"The text \\\n",
    "that is delimited by triple backticks \\\n",
    "is a summary of the methods utilized in a Machine Learning Paper. \\\n",
    "Provide a step-by-step guide in English on how to code these methods \\\n",
    "in the programming language Python, but limit your responses to the feature extraction, \\\n",
    "do not provide any detail related to training and model building. Each step must be detailed enough \\\n",
    "that anyone should be able to reproduce the methods and must not omit any details or steps required. If for whatever reason, the methods being outlined \\\n",
    "do not offer any codable steps, then respond with 'No codable steps found'. \\\n",
    "summary: ```{summary}```\n",
    "\"\"\"\n",
    "\n",
    "# propmpt that inputs a product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "52bc0589-ce41-4f28-9587-788ebae7c881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['summary'], template=\"The text that is delimited by triple backticks is a summary of the methods utilized in a Machine Learning Paper. Provide a step-by-step guide in English on how to code these methods in the programming language Python, but limit your responses to the feature extraction, do not provide any detail related to training and model building. Each step must be detailed enough that anyone should be able to reproduce the methods and must not omit any details or steps required. If for whatever reason, the methods being outlined do not offer any codable steps, then respond with 'No codable steps found'. summary: ```{summary}```\\n\")"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guide_template = ChatPromptTemplate.from_template(template_string)\n",
    "\n",
    "guide_template.messages[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "df0c4df6-ddf9-4d71-9cdb-6972a0a6ca40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['summary']"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm the input variable of the template\n",
    "guide_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "7302b98f-346d-476c-8dff-79b7c602c979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Prepare the images\n",
      "- Obtain a dataset of images of the population being studied\n",
      "- Resize the images to a standard size\n",
      "- Pre-process the images for feature extraction (e.g. convert to grayscale)\n",
      "\n",
      "Step 2: Extract SIFT key points\n",
      "- Use OpenCV library to extract SIFT (Scale-Invariant Feature Transform) key points from the images\n",
      "- These key points represent distinctive areas of an image that can be used for identification\n",
      "- Save the extracted key points for each image\n",
      "\n",
      "Step 3: Match key points\n",
      "- Use a matching algorithm (e.g. FLANN) to match the key points between different images\n",
      "- This allows for identification of individual animals based on their unique coat patterns\n",
      "\n",
      "Step 4: Identify strongly marked texture species\n",
      "- Use a filtering method to select only images with strongly marked textures\n",
      "- This will help improve the accuracy of identification\n",
      "\n",
      "Step 5: Address the challenge of identifying species from remote camera images\n",
      "- Research and understand various methods for recognizing objects in computer vision\n",
      "- Explore Yang's work on using ScSPM (Spatial Pyramid Matching) for object recognition\n",
      "- Determine which method is most suitable for the study\n",
      "\n",
      "Step 6: Implement ScSPM\n",
      "- Use the ScSPM algorithm in the pattern extraction and classification program\n",
      "- This will help with feature extraction and classification of the images\n",
      "\n",
      "Step 7: Extract local feature descriptors\n",
      "- Use dense feature extraction methods (e.g. SIFT and cLBP) to extract local feature descriptors from the images\n",
      "- These descriptors represent the key points and their surrounding areas\n",
      "\n",
      "Step 8: Combine local descriptors\n",
      "- Combine the two types of local descriptors (SIFT and cLBP) to represent the local features sparsely\n",
      "- This will help with feature representation and reduce computational complexity\n",
      "\n",
      "Step 9: Learn the dictionary\n",
      "- Use weighted sparse coding to learn a dictionary for each type of descriptor feature\n",
      "- This will help with identifying similar local features and retaining discriminative information while suppressing noise\n",
      "\n",
      "Step 10: Apply max pooling with SPM\n",
      "- Use Spatial Pyramid Matching (SPM) to construct a global feature that represents the entire image or bounding box\n",
      "- This converts the image or bounding box into a single vector, making it easier for classification\n",
      "\n",
      "Step 11: Classify global image feature\n",
      "- Use linear multi-class SVMs (Support Vector Machines) to classify the global image feature\n",
      "- This will help with accurately identifying the species in the image\n",
      "\n",
      "Step 12: Repeat for all images\n",
      "- Repeat the above steps for all images in the dataset\n",
      "- Save the results for further analysis\n",
      "\n",
      "Step 13: Analyze results\n",
      "- Evaluate the accuracy of the identification method\n",
      "- Make any necessary adjustments or improvements to the steps if needed\n",
      "\n",
      "Step 14: Apply to new images\n",
      "- Once the method has been validated, apply it to new images for identification\n",
      "- This can be done manually or through an automated process\n",
      "\n",
      "Step 15: Congratulations!\n",
      "- You have successfully implemented the feature extraction methods used in the research paper using Python programming language.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# will generate prompt from template string above\n",
    "chain = LLMChain(llm=llm, prompt=guide_template)\n",
    "print(chain.run(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "496784ad-06e1-4e6e-ab76-ab75dc599158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "# prompt template 2\n",
    "code_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"The text \\\n",
    "       that is delimited by triple backticks \\\n",
    "       outlines a step-by-step guide to implement methods outlined in a Machine Learning research paper in Python. \\\n",
    "       Please follow each step and write runnable code in python. The code must work!\n",
    "        ```{guide}```\"\"\"\n",
    ")\n",
    "\n",
    "# Chain 2\n",
    "chain_two = LLMChain(llm=llm, prompt=code_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "b170e9fa-b65f-4d2b-952f-340d28c0bb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_code_chain = SimpleSequentialChain(chains=[chain, chain_two],\n",
    "                                             verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "620dd171-6d0c-4605-98ab-5c0a4361a8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "#Import necessary libraries\n",
      "import cv2\n",
      "import numpy as np\n",
      "\n",
      "#Load the image data\n",
      "img = cv2.imread('coat_pattern.jpg')\n",
      "\n",
      "#Convert image to grayscale\n",
      "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
      "\n",
      "#Use the SIFT algorithm to extract and match key points\n",
      "sift = cv2.SIFT.create()\n",
      "keypoints = sift.detect(gray, None)\n",
      "\n",
      "#Apply a filter to SIFT results\n",
      "strong_keypoints = [keypoint for keypoint in keypoints if keypoint.response > 100]\n",
      "\n",
      "#Implement the ScSPM algorithm\n",
      "yang = cv2.ScSPM.create()\n",
      "keypoints = yang.detect(gray, None)\n",
      "\n",
      "#Extract local feature descriptors densely\n",
      "sift_desc = sift.compute(gray, strong_keypoints)\n",
      "clbp_desc = clbp.compute(gray, strong_keypoints)\n",
      "\n",
      "#Combine SIFT and cLBP descriptors\n",
      "descriptors = np.hstack((sift_desc, clbp_desc))\n",
      "\n",
      "#Learn a dictionary using weighted sparse coding\n",
      "dictionary = cv2.ml.ml.SparseCoder_create()\n",
      "dictionary.setCodeBook(descriptors)\n",
      "\n",
      "#Use max pooling with SPM to construct a global image feature\n",
      "global_feature = dictionary.project(descriptors)\n",
      "\n",
      "#Classify the global image feature using SVM\n",
      "svm = cv2.ml.SVM_create()\n",
      "svm.train(global_feature, cv2.ml.ROW_SAMPLE, labels)\n",
      "\n",
      "#Predict the class of the image\n",
      "svm.predict(img)\n"
     ]
    }
   ],
   "source": [
    "print(simple_code_chain.run(summary))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
